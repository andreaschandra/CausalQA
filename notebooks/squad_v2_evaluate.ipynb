{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc52fb4-25cb-45b3-a34c-d2260c98ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import collections\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bc6f58-7c5d-420f-97a7-96228c631c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    {'prediction_text': '', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.}, \n",
    "    {'prediction_text': 'Beyonce', 'id': '56d2051ce7d4791d0090260b', 'no_answer_probability': 0.}, \n",
    "    {'prediction_text': 'climate change in world', 'id': '5733b5344776f419006610e1', 'no_answer_probability': 0.},\n",
    "    {'prediction_text': 'jakarta', 'id': '5733b5344776f419006610e2', 'no_answer_probability': 0.},\n",
    "    {'prediction_text': 'bandung', 'id': '5733b5344776f419006610e3', 'no_answer_probability': 0.}\n",
    "]\n",
    "references = [\n",
    "    {'answers': {'answer_start': [891], 'text': ['climate change in other world']}, 'id': '5733b5344776f419006610e1'},\n",
    "    {'answers': {'answer_start': [891], 'text': ['jakarta']}, 'id': '5733b5344776f419006610e2'},\n",
    "    {'answers': {'answer_start': [891], 'text': ['bandung']}, 'id': '5733b5344776f419006610e3'},\n",
    "    {'answers': {'answer_start': [], 'text': []}, 'id': '56e10a3be3433e1400422b22'}, \n",
    "    {'answers': {'answer_start': [], 'text': []}, 'id': '56d2051ce7d4791d0090260b'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b173261b-f733-4190-8d73-13f234c13d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_REGEX = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85e815e-2bb9-41ac-aa5a-d566e8f37ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qid_to_has_ans(dataset):\n",
    "    qid_to_has_ans = {}\n",
    "    for article in dataset:\n",
    "        for p in article[\"paragraphs\"]:\n",
    "            for qa in p[\"qas\"]:\n",
    "                qid_to_has_ans[qa[\"id\"]] = bool(qa[\"answers\"][\"text\"])\n",
    "    return qid_to_has_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030f3e88-c912-4f79-b5b1-2a35271fd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return ARTICLES_REGEX.sub(\" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ed3e2e-14e9-4c04-ad44-e6a7ae00af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642cb08c-1c42-4e21-9403-494d70fb3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    \n",
    "    print(\"a_gold\", a_gold)\n",
    "    print(\"a_pred\", a_pred)\n",
    "    \n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    \n",
    "    print(\"gold_toks\", gold_toks)\n",
    "    print(\"pred_toks\", pred_toks)\n",
    "    \n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    print(\"common\", common)\n",
    "    num_same = sum(common.values())\n",
    "    print(\"num_same\", num_same)\n",
    "    \n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"precision\", precision)\n",
    "    print(\"recall\", recall)\n",
    "    print(\"f1\", f1)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad52677-ed8f-485f-8c1b-b9bff822ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_scores(dataset, preds):\n",
    "    exact_scores = {}\n",
    "    f1_scores = {}\n",
    "    for article in dataset:\n",
    "        for p in article[\"paragraphs\"]:\n",
    "            for qa in p[\"qas\"]:\n",
    "                qid = qa[\"id\"]\n",
    "                gold_answers = [t for t in qa[\"answers\"][\"text\"] if normalize_answer(t)]\n",
    "                \n",
    "                print(\"gold_answers\")\n",
    "                print(gold_answers)\n",
    "                \n",
    "                if not gold_answers:\n",
    "                    # For unanswerable questions, only correct answer is empty string\n",
    "                    gold_answers = [\"\"]\n",
    "                if qid not in preds:\n",
    "                    print(f\"Missing prediction for {qid}\")\n",
    "                    continue\n",
    "\n",
    "                a_pred = preds[qid]\n",
    "                \n",
    "                # Take max over all gold answers\n",
    "                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
    "                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
    "                \n",
    "    return exact_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c792d63-6141-449a-8975-df16b028af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
    "    new_scores = {}\n",
    "    for qid, s in scores.items():\n",
    "        pred_na = na_probs[qid] > na_prob_thresh\n",
    "        if pred_na:\n",
    "            new_scores[qid] = float(not qid_to_has_ans[qid])\n",
    "        else:\n",
    "            new_scores[qid] = s\n",
    "    return new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "983f4b3d-1ef6-424d-83a2-62e37952b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
    "    \n",
    "    if not qid_list:\n",
    "        total = len(exact_scores)\n",
    "        return collections.OrderedDict(\n",
    "            [\n",
    "                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n",
    "                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n",
    "                (\"total\", total),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        total = len(qid_list)\n",
    "        return collections.OrderedDict(\n",
    "            [\n",
    "                (\"exact\", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
    "                (\"f1\", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
    "                (\"total\", total),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a0ca8c-aef5-4f9d-b7ce-f12a01fe33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
    "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
    "    cur_score = num_no_ans\n",
    "    best_score = cur_score\n",
    "    best_thresh = 0.0\n",
    "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
    "\n",
    "    for i, qid in enumerate(qid_list):\n",
    "        if qid not in scores:\n",
    "            continue\n",
    "        if qid_to_has_ans[qid]:\n",
    "            diff = scores[qid]\n",
    "        else:\n",
    "            if preds[qid]:\n",
    "                diff = -1\n",
    "            else:\n",
    "                diff = 0\n",
    "\n",
    "        cur_score += diff\n",
    "        \n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            best_thresh = na_probs[qid]\n",
    "    return 100.0 * best_score / len(scores), best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045cccdb-b0a8-4cfb-b29d-a8f291c96f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
    "    \n",
    "    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
    "    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
    "    \n",
    "    main_eval[\"best_exact\"] = best_exact\n",
    "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
    "    main_eval[\"best_f1\"] = best_f1\n",
    "    main_eval[\"best_f1_thresh\"] = f1_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7413ba6-c56f-451f-b3ed-789d5ed02afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute(predictions, references, no_answer_threshold=1.0):\n",
    "    no_answer_probabilities = {p[\"id\"]: p[\"no_answer_probability\"] for p in predictions}\n",
    "    dataset = [{\"paragraphs\": [{\"qas\": references}]}]\n",
    "    predictions = {p[\"id\"]: p[\"prediction_text\"] for p in predictions}\n",
    "    \n",
    "    # print(\"no_answer_probabilities\")\n",
    "    # pprint(no_answer_probabilities)\n",
    "    # print(\"dataset\")\n",
    "    # pprint(dataset)\n",
    "    # print(\"predictions\")\n",
    "    # pprint(predictions)\n",
    "\n",
    "    qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
    "    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "    \n",
    "    # print(\"qid_to_has_ans\")\n",
    "    # pprint(qid_to_has_ans)\n",
    "    # print(\"has_ans_qids\")\n",
    "    # pprint(has_ans_qids)\n",
    "    # print(\"no_ans_qids\")\n",
    "    # pprint(no_ans_qids)\n",
    "\n",
    "    exact_raw, f1_raw = get_raw_scores(dataset, predictions)\n",
    "    print(\"f1_raw\")\n",
    "    pprint(f1_raw)\n",
    "    \n",
    "    exact_thresh = apply_no_ans_threshold(exact_raw, no_answer_probabilities, qid_to_has_ans, no_answer_threshold)\n",
    "    f1_thresh = apply_no_ans_threshold(f1_raw, no_answer_probabilities, qid_to_has_ans, no_answer_threshold)\n",
    "    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "    \n",
    "    print(out_eval)\n",
    "    pprint(out_eval)\n",
    "\n",
    "#     if has_ans_qids:\n",
    "#         has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "#         merge_eval(out_eval, has_ans_eval, \"HasAns\")\n",
    "\n",
    "#     if no_ans_qids:\n",
    "#         no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "#         merge_eval(out_eval, no_ans_eval, \"NoAns\")\n",
    "    \n",
    "    find_all_best_thresh(out_eval, predictions, exact_raw, f1_raw, no_answer_probabilities, qid_to_has_ans)\n",
    "    \n",
    "    return dict(out_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5893561a-5d52-4ebf-8439-2cf4b1f1d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_answers\n",
      "['climate change in other world']\n",
      "a_gold climate change in other world\n",
      "a_pred climate change in world\n",
      "gold_toks ['climate', 'change', 'in', 'other', 'world']\n",
      "pred_toks ['climate', 'change', 'in', 'world']\n",
      "common Counter({'climate': 1, 'change': 1, 'in': 1, 'world': 1})\n",
      "num_same 4\n",
      "precision 1.0\n",
      "recall 0.8\n",
      "f1 0.888888888888889\n",
      "gold_answers\n",
      "['jakarta']\n",
      "a_gold jakarta\n",
      "a_pred jakarta\n",
      "gold_toks ['jakarta']\n",
      "pred_toks ['jakarta']\n",
      "common Counter({'jakarta': 1})\n",
      "num_same 1\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1 1.0\n",
      "gold_answers\n",
      "['bandung']\n",
      "a_gold bandung\n",
      "a_pred bandung\n",
      "gold_toks ['bandung']\n",
      "pred_toks ['bandung']\n",
      "common Counter({'bandung': 1})\n",
      "num_same 1\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1 1.0\n",
      "gold_answers\n",
      "[]\n",
      "a_gold \n",
      "a_pred \n",
      "gold_toks []\n",
      "pred_toks []\n",
      "common Counter()\n",
      "num_same 0\n",
      "gold_answers\n",
      "[]\n",
      "a_gold \n",
      "a_pred Beyonce\n",
      "gold_toks []\n",
      "pred_toks ['beyonce']\n",
      "common Counter()\n",
      "num_same 0\n",
      "f1_raw\n",
      "{'56d2051ce7d4791d0090260b': 0,\n",
      " '56e10a3be3433e1400422b22': 1,\n",
      " '5733b5344776f419006610e1': 0.888888888888889,\n",
      " '5733b5344776f419006610e2': 1.0,\n",
      " '5733b5344776f419006610e3': 1.0}\n",
      "OrderedDict([('exact', 60.0), ('f1', 77.77777777777777), ('total', 5)])\n",
      "OrderedDict([('exact', 60.0), ('f1', 77.77777777777777), ('total', 5)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact': 60.0,\n",
       " 'f1': 77.77777777777777,\n",
       " 'total': 5,\n",
       " 'best_exact': 60.0,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 77.77777777777777,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_compute(predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8727b-3a6a-412a-ae56-564e8632f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
